[workspace]
path = "~/.egirl/workspace"

[local]
endpoint = "http://localhost:8080"      # llama.cpp server
model = "qwen3-vl-32b"                   # for display/logging only
context_length = 32768
max_concurrent = 2                       # you have the VRAM for parallel requests

[local.embeddings]
endpoint = "http://localhost:8082"       # Qwen3-VL-Embedding Python service
model = "qwen3-vl-embedding-2b"
dimensions = 2048
multimodal = true                        # supports text + image embeddings

[routing]
default = "local"
escalation_threshold = 0.4               # confidence below this triggers escalation
always_local = ["memory_search", "memory_get", "greeting", "acknowledgment"]
always_remote = ["code_generation", "code_review", "complex_reasoning"]

[channels.discord]
allowed_channels = ["dm"]                # or specific channel IDs
allowed_users = []                       # set your Discord user ID

[channels.claude_code]
permission_mode = "default"               # local model handles permissions
# model = "sonnet"                       # claude model (sonnet, opus, haiku)
# working_dir = "~/projects/myrepo"      # defaults to cwd
# max_turns = 30                         # safety limit on turns per task

[safety]
enabled = true
audit_log = "{workspace}/audit.log"       # JSONL log of all tool calls
# blocked_patterns = ["extra_regex_here"] # added on top of built-in blocklist
# allowed_paths = ["{workspace}", "~/projects"]  # restrict file ops (empty = no restriction)
# sensitive_patterns = ["secret\\.yaml$"] # added on top of built-in patterns
require_confirmation = false               # block destructive tools until confirmed

[skills]
dirs = ["~/.egirl/skills", "{workspace}/skills"]
